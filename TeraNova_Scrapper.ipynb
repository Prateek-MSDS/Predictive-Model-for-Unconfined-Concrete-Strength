{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517bc5dc",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bce871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq pypdfium2\n",
    "!pip install -Uqq tabula-py\n",
    "!pip install -Uqq pdfplumber\n",
    "!pip install -Uqqq python-dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc613138",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901d836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pypdfium2 as pdfium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import tabula\n",
    "from tabula.io import read_pdf\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c3784",
   "metadata": {},
   "source": [
    "### Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes pdf as an input\n",
    "# converts into text\n",
    "# returns the required fields\n",
    "    \n",
    "def TeraNovaText(folder,file):\n",
    "    pdf_fium = pdfium.PdfDocument(folder+'/'+file) # Read the PDF as a text File\n",
    "    pdf_plumb = pdfplumber.open(folder+'/'+file) # Read the PDF as a text File\n",
    "    \n",
    "    no_of_pages = len(pdf_fium)\n",
    "    \n",
    "    out= []\n",
    "    \n",
    "    for i in range(no_of_pages):\n",
    "        temp_out = []\n",
    "        \n",
    "        page_fium = pdf_fium[i] # pdfium\n",
    "        page_plumb = pdf_plumb.pages[i] # PDF Plumber\n",
    "        \n",
    "        # Load a text page helper\n",
    "        textpage_fium = page_fium.get_textpage() # pdfium\n",
    "    \n",
    "        # Extract text from the whole page\n",
    "        text_fium = textpage_fium.get_text_range() # pdfium\n",
    "        text_plumb = page_plumb.extract_text() # PDF Plumber\n",
    "    \n",
    "        # Split the list with the delimiters\n",
    "        list_fium = text_fium.split('\\r\\n') # pdfium\n",
    "        list_plumb = text_plumb.split('\\n')  # PDF Plumber\n",
    "    \n",
    "    \n",
    "        # Store the requird fields\n",
    "    \n",
    "        # Data Ingestion_datetime\n",
    "        Ingestion_datetime = pd.to_datetime('today').strftime(\"%m/%d/%Y %I:%M:%S %p\")\n",
    "    \n",
    "        # City\n",
    "        City = list_plumb[3].split(',')[0].split(' ')[-1]\n",
    "    \n",
    "        # DATE MOLDED\n",
    "        date_molded_src = \"DATE MOLDED\"\n",
    "        date_molded_src_get_string = [x for x in list_fium if date_molded_src in x]\n",
    "        date_molded_src_lst_str = ''.join(date_molded_src_get_string)\n",
    "        Date_Molded = date_molded_src_lst_str.split(' ')[2]\n",
    "    \n",
    "        # DATE ISSUED\n",
    "        date_issue_src = \"DATE ISSUED\"\n",
    "        date_issue_src_get_string = [x for x in list_fium if date_issue_src in x]\n",
    "        date_issue_src_lst_str = ''.join(date_issue_src_get_string)\n",
    "        Date_Issued = date_issue_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # LAB NUMBER\n",
    "        Lab_Number = date_molded_src_lst_str.split(' ')[5]\n",
    "    \n",
    "        # LOCATION OF PLACEMENT\n",
    "        colon = \":\"\n",
    "        lop_searc = \"LOCATION OF PLACEMENT\"\n",
    "        lop_searc_get_string = [x for x in list_fium if lop_searc in x]\n",
    "        lop_searc_lst_str = ''.join(lop_searc_get_string)\n",
    "\n",
    "        loc = lop_searc_lst_str\n",
    "\n",
    "        if any(c in colon for c in loc):\n",
    "            Location_of_Placement = loc.split(':')[1].strip()\n",
    "        else:\n",
    "            loc = loc.split(' ')\n",
    "            loc.insert(3, ':')\n",
    "            loc = \" \".join(loc) \n",
    "            Location_of_Placement = loc.split(':')[1].strip()\n",
    "        \n",
    "        # CONCRETE SUPPLIER\n",
    "        supplier_src = \"CONCRETE SUPPLIER\"\n",
    "        supplier_src_get_string = [x for x in list_plumb if supplier_src in x]\n",
    "        supplier_src_lst_str = ''.join(supplier_src_get_string)\n",
    "        Concrete_Supplier = supplier_src_lst_str.split('WATER')[0].split(':')[1].strip()\n",
    "    \n",
    "        # MIX ID NO\n",
    "        mixid_src = \"MIX ID NO\"\n",
    "        mixid_src_get_string = [x for x in list_plumb if mixid_src in x]\n",
    "        mixid_src_lst_str = ''.join(mixid_src_get_string)\n",
    "        Mix_ID_No = mixid_src_lst_str.split('UNIT')[0].split(':')[1].strip()\n",
    "    \n",
    "        # WEATHER\n",
    "        weather_src = \"WEATHER\"\n",
    "        weather_src_get_string = [x for x in list_fium if weather_src in x]\n",
    "        if weather_src_get_string:\n",
    "            weather_src_lst_str = ''.join(weather_src_get_string)\n",
    "            Weather = weather_src_lst_str.split(':')[1].strip()\n",
    "            \n",
    "        else:\n",
    "            weather_src = \"W EATHER\"\n",
    "            weather_src_get_string = [x for x in list_fium if weather_src in x]\n",
    "            weather_src_lst_str = ''.join(weather_src_get_string)\n",
    "            Weather = weather_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # TIME MOLDED\n",
    "        time_src = \"TIME MOLDED\"\n",
    "        time_src_get_string = [x for x in list_plumb if time_src in x]\n",
    "        time_src_lst_str = ''.join(time_src_get_string)\n",
    "        Time_Molded = time_src_lst_str.split(' ')[2] + ' ' +time_src_lst_str.split(' ')[3]\n",
    "        if \"ASTM\" in Time_Molded:\n",
    "            Time_Molded = \"\"\n",
    "        else:\n",
    "            Time_Molded = Time_Molded[0:8]\n",
    "    \n",
    "        # AIR CONTENT(%)\n",
    "        aircontent_src = \"AIR CONTENT\"\n",
    "        aircontent_src_get_string = [x for x in list_fium if aircontent_src in x]\n",
    "        aircontent_src_lst_str = ''.join(aircontent_src_get_string)\n",
    "        Air_Content = aircontent_src_lst_str.split(':')[1].strip()\n",
    "        Air_Content = re.sub('[^0-9,.]', '', Air_Content)\n",
    "    \n",
    "        # SLUMP(IN)\n",
    "        slump_src = \"SLUMP(IN)\"\n",
    "        slump_src_get_string = [x for x in list_fium if slump_src in x]\n",
    "        slump_src_lst_str = ''.join(slump_src_get_string)\n",
    "        Slump_space_chck = slump_src_lst_str.split(':')\n",
    "        if len(Slump_space_chck[1])>5:\n",
    "            Slump = Slump_space_chck[1].split(' ')[1].strip()\n",
    "        else:\n",
    "            Slump = Slump_space_chck[1].strip()\n",
    "        \n",
    "        Slump = re.sub('[^0-9,.]', '', Slump)\n",
    "        \n",
    "    \n",
    "        # SIZE & REQUIRED PSI \n",
    "        size_psi_search = \"SIZE:\"\n",
    "        size_psi_get_string = [x for x in list_plumb if size_psi_search in x]\n",
    "        if size_psi_get_string:\n",
    "            size_psi_lst_str = ''.join(size_psi_get_string)\n",
    "            Size = size_psi_lst_str.split('DIAMETER')[0].split(':')[-1].strip()\n",
    "            Required_PSI = size_psi_lst_str.split(' ')[-1].strip()\n",
    "        else:\n",
    "            size_psi_search = \"SIZE\"\n",
    "            size_psi_get_string = [x for x in list_plumb if size_psi_search in x]\n",
    "            size_psi_lst_str = ''.join(size_psi_get_string)\n",
    "            Size = size_psi_lst_str.split('DIAMETER')[0].split('SIZE')[-1].strip()\n",
    "            Required_PSI = size_psi_lst_str.split(' ')[-1].strip()       \n",
    "        \n",
    "        \n",
    "        # WATER ADDED(GALS)    \n",
    "        water_searc = \"GALS\"\n",
    "        water_searc_get_string = [x for x in list_fium if water_searc in x]\n",
    "        water_searc_lst_str = ''.join(water_searc_get_string)\n",
    "        Water_Added_len_check = water_searc_lst_str.split(':')\n",
    "        \n",
    "        if len(Water_Added_len_check[1])>5:\n",
    "            Water_Added = Water_Added_len_check[1].split(' ')[1].strip()\n",
    "        else:\n",
    "            Water_Added = Water_Added_len_check[1].strip()\n",
    "        \n",
    "        Water_Added = re.sub('[^0-9,.]', '', Water_Added)\n",
    "    \n",
    "    \n",
    "        # UNIT WEIGHT(PCF):\n",
    "        unit_searc = \"PCF\"\n",
    "        unit_searc_get_string = [x for x in list_fium if unit_searc in x]\n",
    "        unit_searc_lst_str = ''.join(unit_searc_get_string)\n",
    "        Unit_Weight = unit_searc_lst_str.split(':')[1].strip()\n",
    "        Unit_Weight = re.sub('[^0-9,.]', '', Unit_Weight)\n",
    "    \n",
    "        # AMBIENT TEMP(F):\n",
    "        ambi_temp_searc = \"AMBIENT TEMP\"\n",
    "        ambi_temp_searc_get_string = [x for x in list_fium if ambi_temp_searc in x]\n",
    "        ambi_temp_searc_lst_str = ''.join(ambi_temp_searc_get_string)\n",
    "        Ambient_Temp = ambi_temp_searc_lst_str.split(':')[1].strip()\n",
    "        Ambient_Temp = re.sub('[^0-9]', '', Ambient_Temp)\n",
    "    \n",
    "        # CONCRETE TEMP(F):\n",
    "        concrete_temp_searc = \"CONCRETE TEMP\"\n",
    "        concrete_temp_searc_get_string = [x for x in list_fium if concrete_temp_searc in x]\n",
    "        concrete_temp_searc_lst_str = ''.join(concrete_temp_searc_get_string)\n",
    "        Concrete_Temp = concrete_temp_searc_lst_str.split(':')[1].strip()\n",
    "        Concrete_Temp = re.sub('[^0-9]', '', Concrete_Temp)\n",
    "        \n",
    "        temp_out = [file,Ingestion_datetime,City,Date_Molded,Date_Issued,Lab_Number,Location_of_Placement\n",
    "       ,Concrete_Supplier,Mix_ID_No,Weather,Time_Molded,Air_Content,Slump,Size\n",
    "       ,Required_PSI,Water_Added,Unit_Weight,Ambient_Temp,Concrete_Temp]\n",
    "        \n",
    "        out.append((temp_out))\n",
    "    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea56f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes pdf as an input\n",
    "# converts into a dataframe\n",
    "# returns the test results as a DataFrame\n",
    "\n",
    "def TeraNovaPdf2Table(folder,file,result):\n",
    "    readpdf2df = tabula.io.read_pdf(folder+'/'+file \n",
    "                             , pages='all'\n",
    "                             #,output_format=\"dataframe\"\n",
    "                                   , multiple_tables=True)\n",
    "    \n",
    "    df = pd.concat(readpdf2df)\n",
    "    age_load_read = df.iloc[1:,3].dropna().reset_index().drop(columns=['index']).iloc[:-1,]\n",
    "    \n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('diameter', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('compression', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('age', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    \n",
    "    # Extra Space in the Data Check\n",
    "    space_check = age_load_read.iloc[:,0].str.split(' ').tolist()\n",
    "    \n",
    "    space_check_ref = []\n",
    "    for i in space_check:\n",
    "        for j in i:\n",
    "            if len(i)>4:\n",
    "                i[1]=i[1]+i[2]\n",
    "                i.pop(2)\n",
    "        space_check_ref.append(i)\n",
    "    space_check = space_check_ref\n",
    "    age_load_read = pd.DataFrame(space_check, columns =['Age_Days','Date_Tested','Total_load', 'Unit_load'])\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in range(len(result)):\n",
    "        df_split = np.array_split(age_load_read, len(result))\n",
    "        \n",
    "        result_lab = result[i][5]\n",
    "        result_date_isssue = result[i][4]\n",
    "        \n",
    "        df_sel = df_split[i]\n",
    "        \n",
    "        iterat = df_sel.shape[0]\n",
    "        \n",
    "        lab_nbr = pd.DataFrame({'Lab_Number': result_lab}, index=[0])\n",
    "        date_isu = pd.DataFrame({'Date_Issued': result_date_isssue}, index=[0])\n",
    "        \n",
    "        lab_nbr_itr = pd.DataFrame(np.repeat(lab_nbr.values, iterat, axis=0))\n",
    "        date_isu_itr = pd.DataFrame(np.repeat(date_isu.values, iterat, axis=0))\n",
    "        \n",
    "        lab_nbr_itr.columns = lab_nbr.columns\n",
    "        date_isu_itr.columns = date_isu.columns\n",
    "        \n",
    "        temp_df = pd.concat([lab_nbr_itr,date_isu_itr,age_load_read.iloc[:, age_load_read.columns != 'Total_load']], axis=1, join='inner')\n",
    "        final_df = final_df.append(temp_df).reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    return final_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb08202",
   "metadata": {},
   "source": [
    "### Main Function.  Make sure to change the path of the testing folder befor running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fec7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "df_text = []\n",
    "pdf_df = pd.DataFrame()\n",
    "\n",
    "#path = '/Users/prateek/Documents/Captsone Project/Jupiter Files/TestFolder'\n",
    "\n",
    "os.chdir('/Users/prateek/Documents/Captsone Project/Test Folder')\n",
    "path = '/Users/prateek/Documents/Captsone Project/Test Folder/sample tera nova test'\n",
    "\n",
    "for file in os.listdir(path+'/'):\n",
    "    if file.endswith('.pdf'):\n",
    "        read_datetime = pd.to_datetime('today').strftime(\"%m/%d/%Y %I:%M:%S %p\")\n",
    "        folder = os.path.basename(path)\n",
    "        dir_path = os.path.dirname(path)\n",
    "        \n",
    "        log.append((dir_path,folder,file,read_datetime))  # Appends results for log DF\n",
    "        \n",
    "        result = TeraNovaText(folder,file) # Calls text function\n",
    "        for j in range(len(result)):\n",
    "            df_text.append((result[j]))    # Appends results from text function\n",
    "        \n",
    "        df_out = TeraNovaPdf2Table(folder,file,result) # Calls Dataframe function\n",
    "        pdf_df = pdf_df.append(df_out).reset_index(drop=True) # Appends test results DF\n",
    "        \n",
    "        \n",
    "          \n",
    "\n",
    "# Append Log DataFrame        \n",
    "df_log = pd.DataFrame(log, columns=['Directory','Folder','File', 'Read_DateTime'])\n",
    "\n",
    "# Append DataFrame with Text fields\n",
    "df_page = pd.DataFrame(df_text, columns=['File_Name',\n",
    "                     'Ingestion_datetime',\n",
    "                     'City',\n",
    "                     'Date_Molded',\n",
    "                     'Date_Issued',\n",
    "                     'Lab_Number',\n",
    "                     'Location_of_Placement',\n",
    "                     'Concrete_Supplier',\n",
    "                     'Mix_ID_No',\n",
    "                     'Weather',\n",
    "                     'Time_Molded',\n",
    "                     'Air_Content(%)',\n",
    "                     'Slump(in)',\n",
    "                     'Size',\n",
    "                     'Required_PSI',\n",
    "                     'Water_Added(GALS)',\n",
    "                     'Unit_Weight(PCF)',\n",
    "                     'Ambient_Temp(F)',\n",
    "                     'Concrete_Temp(F)'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9dfead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf574fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16f3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98af8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c4f52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3c6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba717",
   "metadata": {},
   "source": [
    "### Save the Data in Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b11b8ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_page.to_excel('COMPRESSION_TEST_SPECIMENS_REPORT.xlsx', index=False)\n",
    "pdf_df.to_excel('COMPRESSION_TEST_RESULTS.xlsx', index=False)\n",
    "df_log.to_excel('INGESTION_LOG.xlsx', index=False)\n",
    "\n",
    "files_write_dt = datetime.now().strftime('%m/%d/%Y %I:%M:%S %p')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e83740",
   "metadata": {},
   "source": [
    "### Job Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e94099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ends = datetime.strptime(df_page['Ingestion_datetime'].iloc[-1], '%m/%d/%Y %I:%M:%S %p')\n",
    "start = datetime.strptime(df_log['Read_DateTime'][0], '%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "write_end = datetime.strptime(files_write_dt, '%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "diff1 = relativedelta(ends,start)\n",
    "diff2 = relativedelta(write_end,start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "035fe1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Joblog.txt', 'w') as f:\n",
    "    print('=============================================', file=f)\n",
    "    print('  ',date.today().strftime(\"%A,%d %B, %Y\"), file=f)\n",
    "    print('\\t',datetime.now().time().strftime(\"%I:%M:%S %p\"), file=f)\n",
    "    print('=============================================', file=f)\n",
    "    print(\"TeraNova Scrapper Job Completed Successfully\", file=f)\n",
    "    print('=============================================', file=f)\n",
    "    print('Total Files Read:', len(df_log), file=f)\n",
    "    print('Total Pages Ingested:', len(df_page), file=f)\n",
    "    print('=============================================', file=f)\n",
    "    print('First File Read Date Time:',df_log['Read_DateTime'][0], file=f)\n",
    "    print('Last File Ingestion Data Time:',df_page['Ingestion_datetime'].iloc[-1], file=f)\n",
    "    print('=============================================', file=f)\n",
    "    print(\"Reading to Ingesting files took %d hours %d minutes %d seconds\" % (diff1.hours, diff1.minutes, diff1.seconds)\n",
    "          , file=f)\n",
    "    print('=============================================', file=f)\n",
    "    print(\"Total job took %d hours %d minutes %d seconds\" % (diff2.hours, diff2.minutes, diff2.seconds)\n",
    "         , file=f)\n",
    "    print('=============================================', file=f)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a477736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Job Log File!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Check Job Log File!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b027c22",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b9d282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataframes\n",
    "df_page_stg = df_page.copy()\n",
    "pdf_df_stg = pdf_df.copy()\n",
    "\n",
    "# Rename the Columns\n",
    "df_page_stg = df_page_stg.rename(columns={'File_Name': 'ReportFileName'\n",
    "                                          , 'Date_Issued': 'ReportDateIssued'\n",
    "                                          , 'Ingestion_datetime': 'ReportTimeIngested'\n",
    "                                          , 'Ambient_Temp(F)': 'SiteTemperature'\n",
    "                                          , 'Weather': 'SiteWeather'\n",
    "                                          , 'Concrete_Supplier': 'ConcreteSupplier'\n",
    "                                          , 'Location_of_Placement': 'ConcretePlacementLocation'\n",
    "                                          , 'Lab_Number': 'BatchLabNumber'\n",
    "                                          , 'Mix_ID_No': 'BatchMixID'\n",
    "                                          , 'Size': 'BatchSpecimenSize'\n",
    "                                          , 'Concrete_Temp(F)': 'BatchTemperature'\n",
    "                                          , 'Unit_Weight(PCF)': 'BatchUnitWeight'\n",
    "                                          , 'Required_PSI': 'BatchRequiredStrength'\n",
    "                                          , 'Air_Content(%)': 'BatchAirContent'\n",
    "                                          , 'Slump(in)': 'BatchtSlump'\n",
    "                                          , 'Water_Added(GALS)': 'BatchWaterAdded'})\n",
    "\n",
    "\n",
    "\n",
    "pdf_df_stg = pdf_df_stg.rename(columns={'Lab_Number': 'BatchLabNumber'\n",
    "                                          , 'Date_Issued': 'ReportDateIssued'\n",
    "                                          , 'Date_Tested': 'SpecimenDateTested'\n",
    "                                          , 'Unit_load': 'SpecimenMeasuredStrength'\n",
    "                                          , 'Age_Days': 'SpecimentAgeTested'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4df25f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Changing Data Types of Columns for SpecimenFieldReport #########\n",
    "\n",
    "df_page_stg['ReportTimeIngested'] = df_page_stg['ReportTimeIngested'].astype('datetime64[ns]')\n",
    "df_page_stg['ReportDateIssued'] = pd.to_datetime(df_page_stg[\"ReportDateIssued\"], format=\"%d-%b-%y\")\n",
    "\n",
    "df_page_stg['SiteTemperature'] = df_page_stg['SiteTemperature'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_page_stg['BatchTemperature'] = df_page_stg['BatchTemperature'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_page_stg['BatchUnitWeight'] = df_page_stg['BatchUnitWeight'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_page_stg['BatchRequiredStrength'] = df_page_stg['BatchRequiredStrength'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_page_stg['BatchAirContent'] = df_page_stg['BatchAirContent'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_page_stg['BatchtSlump'] = df_page_stg['BatchtSlump'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df_page_stg['BatchWaterAdded'] = df_page_stg['BatchWaterAdded'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "df_page_stg['BatchLabNumber'] = df_page_stg['BatchLabNumber'].astype('int')\n",
    "df_page_stg['SiteTemperature'] = df_page_stg['SiteTemperature'].astype('float')\n",
    "df_page_stg['BatchTemperature'] = df_page_stg['BatchTemperature'].astype('float')\n",
    "df_page_stg['BatchUnitWeight'] = df_page_stg['BatchUnitWeight'].astype('float')\n",
    "df_page_stg['BatchRequiredStrength'] = df_page_stg['BatchRequiredStrength'].astype('float')\n",
    "df_page_stg['BatchAirContent'] = df_page_stg['BatchAirContent'].astype('float')\n",
    "df_page_stg['BatchtSlump'] = df_page_stg['BatchtSlump'].astype('float')\n",
    "df_page_stg['BatchWaterAdded'] = df_page_stg['BatchWaterAdded'].astype('float')\n",
    "\n",
    "df_page_stg[\"Time_Molded\"] = df_page_stg['Time_Molded'].str.lower()\n",
    "df_page_stg['Time_Molded'] = df_page_stg['Time_Molded'].replace({'sm':'am', 'om':'pm'}, regex = True)\n",
    "\n",
    "df_page_stg[\"BatchTimeMolded\"] = df_page_stg[\"Date_Molded\"] + ' ' +df_page_stg[\"Time_Molded\"]\n",
    "df_page_stg['BatchTimeMolded'] = df_page_stg['BatchTimeMolded'].astype('datetime64[ns]')\n",
    "\n",
    "\n",
    "\n",
    "########### Changing Data Types of Columns for SpecimenCompressionTestResult #########\n",
    "\n",
    "pdf_df_stg['ReportDateIssued'] = pd.to_datetime(pdf_df_stg[\"ReportDateIssued\"], format=\"%d-%b-%y\")\n",
    "\n",
    "junk_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in pdf_df_stg['SpecimenDateTested']:\n",
    "    if len(i)!= 9:\n",
    "        junk_df = junk_df.append(pdf_df_stg.loc[pdf_df_stg['SpecimenDateTested'] == i]).reset_index(drop=True)\n",
    "        pdf_df_stg = pdf_df_stg.drop(labels=[pdf_df_stg.loc[pdf_df_stg['SpecimenDateTested'] == i].index.values[0]]\n",
    "                                     , axis=0).reset_index(drop=True)\n",
    "\n",
    "pdf_df_stg['SpecimenDateTested'] = pd.to_datetime(pdf_df_stg[\"SpecimenDateTested\"], format=\"%d-%b-%y\")\n",
    "\n",
    "if junk_df.empty:\n",
    "    pass\n",
    "else:\n",
    "    junk_df['SpecimenDateTested'] = pd.to_datetime(junk_df[\"SpecimenDateTested\"], format=\"%d-%m-%y\")\n",
    "    pdf_df_stg = pd.concat([pdf_df_stg, junk_df], ignore_index=True, sort=False)\n",
    "\n",
    "del junk_df\n",
    "\n",
    "\n",
    "pdf_df_stg['BatchLabNumber'] = pdf_df_stg['BatchLabNumber'].astype('int')\n",
    "#pdf_df_stg['SpecimentAgeTested'] = pdf_df_stg['SpecimentAgeTested'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d59548ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_df_stg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b345fdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_df_stg_new  = pdf_df_stg[pdf_df_stg.groupby('BatchLabNumber')['ReportDateIssued'].transform('max') == pdf_df_stg['ReportDateIssued']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "#pdf_df_stg_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85ae1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf_df_stg_new['SpecimentAgeTested'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5211e091",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_page_stg_new  = df_page_stg[df_page_stg.groupby('BatchLabNumber')['ReportDateIssued'].transform('max') == df_page_stg['ReportDateIssued']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "#df_page_stg_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a2d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_page_stg_new.to_excel('SpecimenFieldReport.xlsx', index=False)\n",
    "pdf_df_stg_new.to_excel('SpecimenCompressionTestResult.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56762a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Success!! Check SpecimenFieldReport and SpecimenCompressionTestResult files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
