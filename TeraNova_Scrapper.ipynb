{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "517bc5dc",
   "metadata": {},
   "source": [
    "### Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04bce871",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq pypdfium2\n",
    "!pip install -Uqq tabula-py\n",
    "!pip install -Uqq pdfplumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc613138",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901d836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pypdfium2 as pdfium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import tabula\n",
    "from tabula.io import read_pdf\n",
    "import pdfplumber\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c3784",
   "metadata": {},
   "source": [
    "### Create functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes pdf as an input\n",
    "# converts into text\n",
    "# returns the required fields\n",
    "    \n",
    "def TeraNovaText(folder,file):\n",
    "    pdf_fium = pdfium.PdfDocument(folder+'/'+file) # Read the PDF as a text File\n",
    "    pdf_plumb = pdfplumber.open(folder+'/'+file) # Read the PDF as a text File\n",
    "    \n",
    "    no_of_pages = len(pdf_fium)\n",
    "    \n",
    "    out= []\n",
    "    \n",
    "    for i in range(no_of_pages):\n",
    "        temp_out = []\n",
    "        \n",
    "        page_fium = pdf_fium[i] # pdfium\n",
    "        page_plumb = pdf_plumb.pages[i] # PDF Plumber\n",
    "        \n",
    "        # Load a text page helper\n",
    "        textpage_fium = page_fium.get_textpage() # pdfium\n",
    "    \n",
    "        # Extract text from the whole page\n",
    "        text_fium = textpage_fium.get_text_range() # pdfium\n",
    "        text_plumb = page_plumb.extract_text() # PDF Plumber\n",
    "    \n",
    "        # Split the list with the delimiters\n",
    "        list_fium = text_fium.split('\\r\\n') # pdfium\n",
    "        list_plumb = text_plumb.split('\\n')  # PDF Plumber\n",
    "    \n",
    "    \n",
    "        # Store the requird fields\n",
    "    \n",
    "        # Data Ingestion_datetime\n",
    "        Ingestion_datetime = pd.to_datetime('today').strftime(\"%d/%m/%Y %I:%M:%S\")\n",
    "    \n",
    "        # City\n",
    "        City = list_plumb[3].split(',')[0].split(' ')[-1]\n",
    "    \n",
    "        # DATE MOLDED\n",
    "        date_molded_src = \"DATE MOLDED\"\n",
    "        date_molded_src_get_string = [x for x in list_fium if date_molded_src in x]\n",
    "        date_molded_src_lst_str = ''.join(date_molded_src_get_string)\n",
    "        Date_Molded = date_molded_src_lst_str.split(' ')[2]\n",
    "    \n",
    "        # DATE ISSUED\n",
    "        date_issue_src = \"DATE ISSUED\"\n",
    "        date_issue_src_get_string = [x for x in list_fium if date_issue_src in x]\n",
    "        date_issue_src_lst_str = ''.join(date_issue_src_get_string)\n",
    "        Date_Issued = date_issue_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # LAB NUMBER\n",
    "        Lab_Number = date_molded_src_lst_str.split(' ')[5]\n",
    "    \n",
    "        # LOCATION OF PLACEMENT\n",
    "        colon = \":\"\n",
    "        lop_searc = \"LOCATION OF PLACEMENT\"\n",
    "        lop_searc_get_string = [x for x in list_fium if lop_searc in x]\n",
    "        lop_searc_lst_str = ''.join(lop_searc_get_string)\n",
    "\n",
    "        loc = lop_searc_lst_str\n",
    "\n",
    "        if any(c in colon for c in loc):\n",
    "            Location_of_Placement = loc.split(':')[1].strip()\n",
    "        else:\n",
    "            loc = loc.split(' ')\n",
    "            loc.insert(3, ':')\n",
    "            loc = \" \".join(loc) \n",
    "            Location_of_Placement = loc.split(':')[1].strip()\n",
    "        \n",
    "        # CONCRETE SUPPLIER\n",
    "        supplier_src = \"CONCRETE SUPPLIER\"\n",
    "        supplier_src_get_string = [x for x in list_fium if supplier_src in x]\n",
    "        supplier_src_lst_str = ''.join(supplier_src_get_string)\n",
    "        Concrete_Supplier = supplier_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # MIX ID NO\n",
    "        mixid_src = \"MIX ID NO\"\n",
    "        mixid_src_get_string = [x for x in list_fium if mixid_src in x]\n",
    "        mixid_src_lst_str = ''.join(mixid_src_get_string)\n",
    "        Mix_ID_No = mixid_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # WEATHER\n",
    "        weather_src = \"WEATHER\"\n",
    "        weather_src_get_string = [x for x in list_fium if weather_src in x]\n",
    "        if weather_src_get_string:\n",
    "            weather_src_lst_str = ''.join(weather_src_get_string)\n",
    "            Weather = weather_src_lst_str.split(':')[1].strip()\n",
    "            \n",
    "        else:\n",
    "            weather_src = \"W EATHER\"\n",
    "            weather_src_get_string = [x for x in list_fium if weather_src in x]\n",
    "            weather_src_lst_str = ''.join(weather_src_get_string)\n",
    "            Weather = weather_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # TIME MOLDED\n",
    "        time_src = \"TIME MOLDED\"\n",
    "        time_src_get_string = [x for x in list_plumb if time_src in x]\n",
    "        time_src_lst_str = ''.join(time_src_get_string)\n",
    "        Time_Molded = time_src_lst_str.split(' ')[2] + ' ' +time_src_lst_str.split(' ')[3]\n",
    "    \n",
    "        # AIR CONTENT(%)\n",
    "        aircontent_src = \"AIR CONTENT\"\n",
    "        aircontent_src_get_string = [x for x in list_fium if aircontent_src in x]\n",
    "        aircontent_src_lst_str = ''.join(aircontent_src_get_string)\n",
    "        Air_Content = aircontent_src_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # SLUMP(IN)\n",
    "        slump_src = \"SLUMP(IN)\"\n",
    "        slump_src_get_string = [x for x in list_fium if slump_src in x]\n",
    "        slump_src_lst_str = ''.join(slump_src_get_string)\n",
    "        Slump_space_chck = slump_src_lst_str.split(':')\n",
    "        if len(Slump_space_chck[1])>5:\n",
    "            Slump = Slump_space_chck[1].split(' ')[1].strip()\n",
    "        else:\n",
    "            Slump = Slump_space_chck[1].strip()\n",
    "        \n",
    "    \n",
    "        # SIZE & REQUIRED PSI \n",
    "        size_psi_search = \"NO. SUBMITTED\"\n",
    "        size_psi_get_string = [x for x in list_fium if size_psi_search in x]\n",
    "        size_psi_lst_str = ''.join(size_psi_get_string)\n",
    "        size_psi = size_psi_lst_str.split(':')[2].strip()\n",
    "    \n",
    "        if len(size_psi_lst_str.split(':')[1])<8:\n",
    "            if len(size_psi)>18:\n",
    "                Size = size_psi[0:5]\n",
    "                Required_PSI = size_psi[25:].strip()    \n",
    "            else:\n",
    "                Size = size_psi[0:5]\n",
    "                Required_PSI = size_psi_lst_str.split(':')[3].strip()\n",
    "        else:\n",
    "            get_size = size_psi_lst_str.split(':')[1].strip()\n",
    "            Size = get_size[6:12].strip()\n",
    "            Required_PSI = size_psi_lst_str.split(':')[2].strip()\n",
    "        \n",
    "        # WATER ADDED(GALS)    \n",
    "        water_searc = \"GALS\"\n",
    "        water_searc_get_string = [x for x in list_fium if water_searc in x]\n",
    "        water_searc_lst_str = ''.join(water_searc_get_string)\n",
    "        Water_Added_len_check = water_searc_lst_str.split(':')\n",
    "        if len(Water_Added_len_check[1])>5:\n",
    "            Water_Added = Water_Added_len_check[1].split(' ')[1].strip()\n",
    "        else:\n",
    "            Water_Added = Water_Added_len_check[1].strip()\n",
    "    \n",
    "        # UNIT WEIGHT(PCF):\n",
    "        unit_searc = \"PCF\"\n",
    "        unit_searc_get_string = [x for x in list_fium if unit_searc in x]\n",
    "        unit_searc_lst_str = ''.join(unit_searc_get_string)\n",
    "        Unit_Weight = unit_searc_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # AMBIENT TEMP(F):\n",
    "        ambi_temp_searc = \"AMBIENT TEMP\"\n",
    "        ambi_temp_searc_get_string = [x for x in list_fium if ambi_temp_searc in x]\n",
    "        ambi_temp_searc_lst_str = ''.join(ambi_temp_searc_get_string)\n",
    "        Ambient_Temp = ambi_temp_searc_lst_str.split(':')[1].strip()\n",
    "    \n",
    "        # CONCRETE TEMP(F):\n",
    "        concrete_temp_searc = \"CONCRETE TEMP\"\n",
    "        concrete_temp_searc_get_string = [x for x in list_fium if concrete_temp_searc in x]\n",
    "        concrete_temp_searc_lst_str = ''.join(concrete_temp_searc_get_string)\n",
    "        Concrete_Temp = concrete_temp_searc_lst_str.split(':')[1].strip()\n",
    "        \n",
    "        temp_out = [file,Ingestion_datetime,City,Date_Molded,Date_Issued,Lab_Number,Location_of_Placement\n",
    "       ,Concrete_Supplier,Mix_ID_No,Weather,Time_Molded,Air_Content,Slump,Size\n",
    "       ,Required_PSI,Water_Added,Unit_Weight,Ambient_Temp,Concrete_Temp]\n",
    "        \n",
    "        out.append((temp_out))\n",
    "    \n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea56f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes pdf as an input\n",
    "# converts into a dataframe\n",
    "# returns the test results as a DataFrame\n",
    "\n",
    "def TeraNovaPdf2Table(folder,file,result):\n",
    "    readpdf2df = tabula.io.read_pdf(folder+'/'+file \n",
    "                             , pages='all'\n",
    "                             #,output_format=\"dataframe\"\n",
    "                                   , multiple_tables=True)\n",
    "    \n",
    "    df = pd.concat(readpdf2df)\n",
    "    age_load_read = df.iloc[1:,3].dropna().reset_index().drop(columns=['index']).iloc[:-1,]\n",
    "    \n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('diameter', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('compression', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    drp_extra = age_load_read[age_load_read.iloc[:,0].str.contains('age', case=False, na=False)].index\n",
    "    age_load_read = age_load_read.drop(drp_extra)\n",
    "    \n",
    "    # Extra Space in the Data Check\n",
    "    space_check = age_load_read.iloc[:,0].str.split(' ').tolist()\n",
    "    \n",
    "    space_check_ref = []\n",
    "    for i in space_check:\n",
    "        for j in i:\n",
    "            if len(i)>4:\n",
    "                i[1]=i[1]+i[2]\n",
    "                i.pop(2)\n",
    "        space_check_ref.append(i)\n",
    "    space_check = space_check_ref\n",
    "    age_load_read = pd.DataFrame(space_check, columns =['Age_Days','Date_Tested','Total_load', 'Unit_load'])\n",
    "    final_df = pd.DataFrame()\n",
    "    for i in range(len(result)):\n",
    "        df_split = np.array_split(age_load_read, len(result))\n",
    "        \n",
    "        result_lab = result[i][5]\n",
    "        result_date_isssue = result[i][4]\n",
    "        \n",
    "        df_sel = df_split[i]\n",
    "        \n",
    "        iterat = df_sel.shape[0]\n",
    "        \n",
    "        lab_nbr = pd.DataFrame({'Lab_Number': result_lab}, index=[0])\n",
    "        date_isu = pd.DataFrame({'Date_Issued': result_date_isssue}, index=[0])\n",
    "        \n",
    "        lab_nbr_itr = pd.DataFrame(np.repeat(lab_nbr.values, iterat, axis=0))\n",
    "        date_isu_itr = pd.DataFrame(np.repeat(date_isu.values, iterat, axis=0))\n",
    "        \n",
    "        lab_nbr_itr.columns = lab_nbr.columns\n",
    "        date_isu_itr.columns = date_isu.columns\n",
    "        \n",
    "        temp_df = pd.concat([lab_nbr_itr,date_isu_itr,age_load_read.iloc[:, age_load_read.columns != 'Total_load']], axis=1, join='inner')\n",
    "        final_df = final_df.append(temp_df).reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    return final_df\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb08202",
   "metadata": {},
   "source": [
    "### Main Function.  Make sure to change the path of the testing folder befor running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fec7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "df_text = []\n",
    "pdf_df = pd.DataFrame()\n",
    "\n",
    "#path = '/Users/prateek/Documents/Captsone Project/Jupiter Files/TestFolder'\n",
    "os.chdir('/Users/prateek/Documents/Captsone Project/Test Folder')\n",
    "path = '/Users/prateek/Documents/Captsone Project/Test Folder/sample tera nova test'\n",
    "\n",
    "for file in os.listdir(path+'/'):\n",
    "    if file.endswith('.pdf'):\n",
    "        read_datetime = pd.to_datetime('today').strftime(\"%d/%m/%Y %I:%M:%S\")\n",
    "        folder = os.path.basename(path)\n",
    "        dir_path = os.path.dirname(path)\n",
    "        \n",
    "        log.append((dir_path,folder,file,read_datetime))  # Appends results for log DF\n",
    "        \n",
    "        result = TeraNovaText(folder,file) # Calls text function\n",
    "        for j in range(len(result)):\n",
    "            df_text.append((result[j]))    # Appends results from text function\n",
    "        \n",
    "        df_out = TeraNovaPdf2Table(folder,file,result) # Calls Dataframe function\n",
    "        pdf_df = pdf_df.append(df_out).reset_index(drop=True) # Appends test results DF\n",
    "        \n",
    "        \n",
    "          \n",
    "\n",
    "# Append Log DataFrame        \n",
    "df_log = pd.DataFrame(log, columns=['Directory','Folder','File', 'Read_DateTime'])\n",
    "\n",
    "# Append DataFrame with Text fields\n",
    "df_page = pd.DataFrame(df_text, columns=['File_Name',\n",
    "                     'Ingestion_datetime',\n",
    "                     'City',\n",
    "                     'Date_Molded',\n",
    "                     'Date_Issued',\n",
    "                     'Lab_Number',\n",
    "                     'Location_of_Placement',\n",
    "                     'Concrete_Supplier',\n",
    "                     'Mix_ID_No',\n",
    "                     'Weather',\n",
    "                     'Time_Molded',\n",
    "                     'Air_Content(%)',\n",
    "                     'Slump(in)',\n",
    "                     'Size',\n",
    "                     'Required_PSI',\n",
    "                     'Water_Added(GALS)',\n",
    "                     'Unit_Weight(PCF)',\n",
    "                     'Ambient_Temp(F)',\n",
    "                     'Concrete_Temp(F)'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9dfead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf574fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c16f3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98af8e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c4f52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3c6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eba717",
   "metadata": {},
   "source": [
    "### Save the Data in Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b11b8ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!!\n"
     ]
    }
   ],
   "source": [
    "df_page.to_excel('COMPRESSION_TEST_SPECIMENS_REPORT.xlsx', index=False)\n",
    "pdf_df.to_excel('COMPRESSION_TEST_RESULTS.xlsx', index=False)\n",
    "df_log.to_excel('INGESTION_LOG.xlsx', index=False)\n",
    "\n",
    "print(\"Success!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573eacb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
