{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJKGb9_m3QTv"
      },
      "source": [
        "# **Table names used:**\n",
        "\n",
        "\n",
        "**first_table**  :   SpecimenFieldReport.\n",
        "\n",
        "**second_table**  :   SpecimenCompressionTestResult\n",
        "\n",
        "**log_file**  :  Log table to store the invalid files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkxFJu5KrhJo"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buOTLc_-MAos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65d63055-b9e9-43cf-d65b-aedd16c6f079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u3c_25hMsHg"
      },
      "outputs": [],
      "source": [
        "!pip install pdfplumber -q\n",
        "!pip install regex\n",
        "!pip install camelot-py[base]\n",
        "!apt-get install ghostscript\n",
        "!pip uninstall -y 'PyPDF2>=3.0'\n",
        "!pip install 'PyPDF2<3.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1X7D7MxMvwC"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import camelot as cam\n",
        "import pandas as pd\n",
        "from numpy.core.fromnumeric import transpose\n",
        "from pathlib import Path\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLusI4Qe8B3q"
      },
      "source": [
        "# ***SCRAPING DATA INTO FIRST TABLE***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDcEShG3NNhQ"
      },
      "source": [
        "**Function to extract the data into First table**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL1rN-z7NMlX"
      },
      "outputs": [],
      "source": [
        "def extract_first_table(path, text):\n",
        "\n",
        "  global log_file\n",
        "\n",
        "  \n",
        "  Project_Name = Mix_id = City = Date_casted = Date_Report_Issued = Weather = Temperature_Ambient = concrete_supplier = ''\n",
        "  Req_load = Location = Structural_Element = Mix_id = Building_Height = Water_Added = Admixtures_noted = specimen_size = ''\n",
        "\n",
        "\n",
        "  # Fetching the file name\n",
        "  File_name = Path(path).stem\n",
        "\n",
        "\n",
        "  for row in text.split('\\n'):\n",
        "    \n",
        "    #City\n",
        "    if Project_Name == '':\n",
        "      x_proj = re.search(\"Proj\", row)\n",
        "      \n",
        "      if x_proj:\n",
        "        string_proj = row.replace(\" \", \"\")   #print(string_proj)\n",
        "        start = ','\n",
        "        end = ','\n",
        "        City = (string_proj.split(start))[1].split(end)[0]\n",
        "\n",
        "        #Project Name                                                                    # Can it be made a default value?\n",
        "        start = 'Project:'\n",
        "        end = '–'\n",
        "        p_name = (string_proj.split(start))[1].split(end)[0]\n",
        "        Project_Name = p_name.replace('x', ' x ')\n",
        "\n",
        "      else:\n",
        "        City = Project_Name = np.nan\n",
        "\n",
        "\n",
        "    #Date & Cylinder Pick-up Date:\n",
        "    if Date_casted == '':\n",
        "      x_d = re.search(\"Date:\", row)\n",
        "      if x_d:\n",
        "        x_pick = re.search(\"Pick-up Date:\", row)\n",
        "        if x_pick:\n",
        "          Cylinder_pick_up_date = row.split(\"Pick-up Date:\",1)[1].strip()\n",
        "        else:\n",
        "          Date_casted = row.split(\"Date:\",1)[1].strip()\n",
        "      else:\n",
        "        Date_casted == ''\n",
        "\n",
        "\n",
        "    #Date Report Issued & Date tested\n",
        "    x_date = re.search(\"Date\", row)\n",
        "    x_D = re.search(\"DATE\", row)\n",
        "    if x_date:\n",
        "      x_rep = re.search(\"Date Report Issued:\", row)\n",
        "      if x_rep:\n",
        "        Date_Report_Issued = row.split(\"Date Report Issued:\",1)[1].strip()\n",
        "  \n",
        "    elif x_D:\n",
        "      Date_Tested = row.split(\"DATE TESTED\",1)[1].strip()\n",
        "\n",
        "\n",
        "    #Weather\n",
        "    if Weather == '':\n",
        "      x_w = re.search(\"Weather\", row)\n",
        "      if x_w:\n",
        "        string_weather = row.replace(\" \", \"\")   #print(string_proj)\n",
        "        start = 'Weather:'\n",
        "        end = 'Air'\n",
        "        Weather = (string_weather.split(start))[1].split(end)[0].strip()\n",
        "      else:\n",
        "        Weather = ''\n",
        "\n",
        "  \n",
        "    #Temperature_Ambient\n",
        "    if Temperature_Ambient == '':\n",
        "      x = re.search(\"Air\", row)\n",
        "      if x:\n",
        "        string_air = row.replace(\" \", \"\")  \n",
        "\n",
        "        AirTemp = re.search(\"AirTemperature:\", string_air)\n",
        "        if AirTemp:\n",
        "          Temperature_Ambient = string_air[AirTemp.end():][:-3]    #else: Temperatures_Ambient = ''\n",
        "      else:\n",
        "        Temperature_Ambient = ''\n",
        "\n",
        "\n",
        "    #Building Height/Number of Floors\n",
        "    if Building_Height == '':\n",
        "      x_bld_ht = re.search(\"Building height\", row)\n",
        "      if x_bld_ht:\n",
        "        Building_Height = \"Building Height is found\"\n",
        "      else:\n",
        "        Building_Height = np.nan\n",
        "\n",
        "\n",
        "    #Structural Element\n",
        "    if Structural_Element == '':\n",
        "      x_str_el = re.search(\"Structural\", row)\n",
        "      if x_str_el:\n",
        "        Structural_Element = \"Structural Element is found\"\n",
        "      else:\n",
        "        Structural_Element = np.nan\n",
        "\n",
        "\n",
        "    #Location of Placement\n",
        "    if Location == '':\n",
        "      x_loc = re.search(\"Locations:\", row)\n",
        "      if x_loc:\n",
        "        Location = row.split(\"Locations:\",1)[1].strip()            \n",
        "      else:\n",
        "        Location = ''\n",
        "\n",
        "    \n",
        "    #Mix ID\n",
        "    if Mix_id == '':\n",
        "      x_mix = re.search(\"Mix\", row)\n",
        "      if x_mix:\n",
        "        string_mix = row.replace(\" \", \"\")\n",
        "        start = 'Mix'\n",
        "        end = '.'\n",
        "        mixid = (string_mix.split(start))[1].split(end)[0]\n",
        "        Mix_id = re.sub('\\D', '', mixid)\n",
        "      else:\n",
        "        Mix_id = ''\n",
        "\n",
        "\n",
        "    #Water Added\n",
        "    if Water_Added == '':\n",
        "      x_water = re.search(\"Water\", row, re.IGNORECASE)\n",
        "      if x_water:\n",
        "        Water_Added = \"Water found\"\n",
        "      else:\n",
        "        Water_Added = ''\n",
        "\n",
        "    \n",
        "    #Any Admixtures noted, \n",
        "    if Admixtures_noted == '': \n",
        "      x_ad_mix = re.search(\"Admix\", row, re.IGNORECASE)\n",
        "\n",
        "      if x_ad_mix:\n",
        "        Admixtures_noted = \"Admixtures noted is found\"\n",
        "      else:\n",
        "        Admixtures_noted = np.nan\n",
        "\n",
        "\n",
        "    #Required Load\n",
        "    if Req_load == '':\n",
        "      x = re.search(\"CLASS\", row)\n",
        "      if x:\n",
        "        string_req_load = row.replace(\" \", \"\")  \n",
        "        start = 'CLASS(PSI)'\n",
        "        end = 'Max/Min'\n",
        "        Req_load = (string_req_load.split(start))[1].split(end)[0].strip()\n",
        "\n",
        "      else:\n",
        "        Req_load = ''\n",
        "\n",
        "\n",
        "    #Concrete supplier\n",
        "    if concrete_supplier == '':\n",
        "      x = re.search(\"Supplier\", row)\n",
        "      if x:\n",
        "        string_supplier = row.replace(\" \", \"\")  \n",
        "        start = 'ConcreteSupplier:'\n",
        "        end = 'CylinderPick'\n",
        "        concrete_supplier = (string_supplier.split(start))[1].split(end)[0].strip()\n",
        "      else:\n",
        "        concrete_supplier = ''\n",
        "    \n",
        " \n",
        "    #Specimen size\n",
        "    if specimen_size == '':\n",
        "      x = re.search(\"Field Set\", row)\n",
        "  \n",
        "      if x:\n",
        "        string_size = row.replace(\" \", \"\")  \n",
        "        start = 'FieldSet('\n",
        "        end = ')'\n",
        "        size = (string_size.split(start))[1].split(end)[0].strip().replace('\"', \"\")\n",
        "        specimen_size = re.sub(\"[^A-Z0-9]\", \"\", size,0,re.IGNORECASE)\n",
        "      else:\n",
        "        specimen_size = ''\n",
        "\n",
        "\n",
        "\n",
        "  #### IMPORTING FIELDS TO A DATA FRAME ####\n",
        "  Table_1_header = [\"ReportFileName\", \"Project_Name\", \"City\", \"Date_Molded\", \"ReportDateIssued\", \"SiteWeather\", \"SiteTemperature\", \"ConcreteSupplier\", \"BatchRequiredStrength\", \n",
        "                  \"ConcretePlacementLocation\", \"Structural_Element\", \"BatchMixID\", \"Building_Height\", \"BatchWaterAdded\", \"Admixtures_noted\", \"BatchSpecimenSize\" ]\n",
        "\n",
        "  Values = [File_name, Project_Name, City, Date_casted, Date_Report_Issued, Weather, Temperature_Ambient, concrete_supplier, Req_load, \n",
        "         Location, Structural_Element, Mix_id, Building_Height, Water_Added, Admixtures_noted, specimen_size]\n",
        "\n",
        "  new_row_table_1 = pd.DataFrame(Values, Table_1_header).T\n",
        "\n",
        "  \n",
        "\n",
        "  ##############  Concrete Field Inspection and Test Results  ################\n",
        "\n",
        "\n",
        "  all_tables = cam.read_pdf(path, pages = 'all', flavor = 'lattice', copy_text=['h'])\n",
        "\n",
        "  x_table_field = []\n",
        "\n",
        "  invalid_time_log = []\n",
        "\n",
        "\n",
        "  for i in range(all_tables.n):\n",
        "\n",
        "    if 0 < i <= repeated:\n",
        "      \n",
        "      tab = all_tables[i].df\n",
        "\n",
        "      #Drop unwanted columns:\n",
        "      tab_new = tab.drop(labels=[1,2,3], axis=0)  \n",
        "\n",
        "      #Converting the dataframe to string\n",
        "      tab_new = tab_new.astype(str)\n",
        "\n",
        "      # Replacing empty spaces with Null Values\n",
        "      tab_new = tab_new.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "      # Dropping the columns with all Null values:\n",
        "      tab_new = tab_new.dropna(axis=1, how='all')\n",
        "\n",
        "      # Transpose\n",
        "      tab_tran = tab_new.T\n",
        "\n",
        "      # Converting first row into header:\n",
        "      headers = tab_tran.iloc[0]\n",
        "      tab_field = pd.DataFrame(tab_tran.values[1:], columns = headers) \n",
        "\n",
        "      tab_field['Field Set (4”x8”) #'] = tab_field['Field Set (4”x8”) #'].str.replace(r'[^\\d.]+', '')\n",
        "\n",
        "      # Replacing the names of first column elements to 'Lab_No' and 'Density' fields:\n",
        "      df_Table_2 =  tab_field\n",
        "      df_Table_2 = df_Table_2.rename(columns={'Field Set (4”x8”) #': 'Lab_no'})\n",
        "      df_Table_2 = df_Table_2.rename(columns={'Density (lbs/cu ft)': 'Unit_Weight (lbs/cu ft)'}) \n",
        "\n",
        "\n",
        "      # Chaging the format of 'Time Discharge' and making the values NULL, for invalid time formats\n",
        "      # And storing them in logfiles.\n",
        "      df_Table_2['Time Discharge'] = df_Table_2['Time Discharge'].str.replace('\\D', '', regex=True)     # Removind values other than digits\n",
        "      \n",
        "      for index in range(len(df_Table_2)):\n",
        "\n",
        "        time_dis = df_Table_2.loc[index, 'Time Discharge']\n",
        "        \n",
        "        if len(time_dis) == 4:\n",
        "          time_conv = datetime.strptime(time_dis, '%H%M').time()\n",
        "          t = time_conv.strftime('%H:%M:%S')\n",
        "          t1 = time_conv.strftime('%I:%M %p')\n",
        "          df_Table_2.loc[index,'Time Discharge'] = t1\n",
        "          df_Table_2.loc[index, 'Batch_Time_Molded'] = t\n",
        "\n",
        "        else:\n",
        "          lab_number = df_Table_2.loc[index,'Lab_no']\n",
        "          df_Table_2.loc[index,'Time Discharge'] = ''                                                                                     \n",
        "          df_Table_2.loc[index, 'Batch_Time_Molded'] =''                                                                                  \n",
        "          Comment = 'Passed NULL value for Time molded for the lab no', lab_number, ' due to incorrect time format'\n",
        "          log_file = log_file.append({'File Name' : File_name, 'Comment' : Comment}, ignore_index = True)\n",
        "\n",
        "\n",
        "      # Rnaming the columns\n",
        "      df_Table_2.rename(columns={\"Air Content (%)\": \"BatchAirContent\"}, inplace=True)\n",
        "      df_Table_2.rename(columns={\"Slump (inches)\": \"BatchtSlump\"}, inplace=True)\n",
        "      df_Table_2.rename(columns={\"Unit_Weight (lbs/cu ft)\": \"BatchUnitWeight\"}, inplace=True)\n",
        "      df_Table_2.rename(columns={\"Concrete Temperature (°F)\": \"BatchTemperature\"}, inplace=True)\n",
        "      df_Table_2.rename(columns={\"Time Discharge\": \"Time_Molded\"}, inplace=True)\n",
        "\n",
        "\n",
        "      if i == 1:\n",
        "        x_table_field = df_Table_2\n",
        "      else:\n",
        "        x_table_field = pd.concat([x_table_field, df_Table_2], axis=0, ignore_index=True)\n",
        "\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "\n",
        "  # MERGING BOTH THE TABLES\n",
        "  \n",
        "  tables_merged = pd.concat([new_row_table_1, x_table_field], axis=1)\n",
        "\n",
        "  tables_merged[\"ReportFileName\"] = File_name \n",
        "  tables_merged[\"Project_Name\"] = Project_Name\n",
        "  tables_merged[\"City\"] = City\n",
        "  tables_merged[\"Date_Molded\"] = Date_casted\n",
        "  tables_merged[\"ReportDateIssued\"] = Date_Report_Issued\n",
        "  tables_merged[\"SiteWeather\"] = Weather\n",
        "  tables_merged[\"SiteTemperature\"] = Temperature_Ambient\n",
        "  tables_merged[\"ConcreteSupplier\"] = concrete_supplier\n",
        "  tables_merged[\"BatchRequiredStrength\"] = Req_load\n",
        "  tables_merged[\"ConcretePlacementLocation\"] = Location\n",
        "  tables_merged[\"Structural_Element\"] = Structural_Element\n",
        "  tables_merged[\"BatchMixID\"] = Mix_id\n",
        "  tables_merged[\"Building_Height\"] = Building_Height\n",
        "  tables_merged[\"BatchWaterAdded\"] = Water_Added\n",
        "  tables_merged[\"Admixtures_noted\"] = Admixtures_noted\n",
        "  tables_merged[\"BatchSpecimenSize\"] = specimen_size\n",
        "\n",
        "   \n",
        "  # Changing the date format of 'Date_Casted' field:\n",
        "  dt_cas = datetime.strptime(Date_casted, '%B %d, %Y').date()\n",
        "  new_date_format = dt_cas.strftime('%Y-%m-%d')\n",
        "\n",
        " \n",
        "  # Batch_time_molded field:\n",
        "\n",
        "  for count in range(len(tables_merged)):\n",
        "    \n",
        "    if tables_merged.loc[count, 'Batch_Time_Molded'] != '':\n",
        "      tables_merged.loc[count, 'Batch_Time_Molded'] = new_date_format + ' ' + tables_merged.loc[count, 'Batch_Time_Molded']\n",
        "\n",
        "  first_table = tables_merged\n",
        "\n",
        "\n",
        "  return first_table\n",
        "\n",
        "  return log_file\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJNSaG9Ea4cG"
      },
      "source": [
        "# ***SCRAPING DATA INTO SECOND TABLE***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRll15GqaqX9"
      },
      "outputs": [],
      "source": [
        "def extract_second_table(path, text):\n",
        "\n",
        "\n",
        "    \n",
        "  read_all_tables = cam.read_pdf(path, pages = 'all', flavor = 'lattice', copy_text=['h'])\n",
        "\n",
        "\n",
        "  x_table = []\n",
        "\n",
        "\n",
        "  for i in range(read_all_tables.n):\n",
        "\n",
        "    if i <= repeated:\n",
        "      continue\n",
        "    else:\n",
        "\n",
        "      tab = read_all_tables[i].df\n",
        "\n",
        "      #Converting the dataframe to string\n",
        "      tab = tab.astype(str)\n",
        "\n",
        "      # Replacing empty spaces with Null Values\n",
        "      tab = tab.replace(r'^\\s*$', np.nan, regex=True)\n",
        "\n",
        "      # Dropping the columns with all Null values:\n",
        "      tab = tab.dropna(axis=1, how='all')\n",
        "\n",
        "      #Drop unwanted columns:\n",
        "      tab_new = tab.drop(labels=[2,3,6,8,9,10,11], axis=0)\n",
        "      tab_new_trans = tab_new.T\n",
        "\n",
        "      # Converting first row into header and Changing name of the first header element\n",
        "      headers = tab_new_trans.iloc[0]\n",
        "      tab_final = pd.DataFrame(tab_new_trans.values[1:], columns = headers)\n",
        "      tab_final.columns.values[0:1] =['Lab_No']\n",
        "\n",
        "      # Changing the name of the field and converting the unit:\n",
        "      tab_final = tab_final.rename(columns={'PSI': 'Unit_Load (PSI)'})\n",
        "\n",
        "      #tab_final['Unit_Load (PSI)'] = pd.to_numeric(tab_final['Unit_Load (PSI)'])\n",
        "      #tab_final['Unit_Load (PSI)'] = (tab_final['Unit_Load (PSI)'] * 0.4535).round(decimals=2)\n",
        "      #tab_final = tab_final.rename(columns={'Unit_Load (PSI)': 'Unit_load (kgs/sq in)'})\n",
        "\n",
        "      tab_final = tab_final.rename(columns={'Unit_Load (PSI)': 'Unit_load (lbs/sq in)'})\n",
        "      tab_final['Lab_No'] = tab_final['Lab_No'].str.replace(r'[^\\d.]+', '')\n",
        "\n",
        "      # Dropping the rows with if any Null values:\n",
        "      tab_final = tab_final.dropna()\n",
        "\n",
        "      if i == repeated + 1:\n",
        "        x_table = tab_final                                                  \n",
        "      else:\n",
        "        x_table = pd.concat([x_table, tab_final], axis=0, ignore_index=True)\n",
        "\n",
        "\n",
        "  # Renaming the columns\n",
        "  x_table.rename(columns={\"CYLINDER #\": \"CylinderTestID\"}, inplace=True)\n",
        "  x_table.rename(columns={\"DATE TESTED\": \"SpecimenDateTested\"}, inplace=True)\n",
        "  x_table.rename(columns={\"TEST AT (days)\": \"SpecimentAgeTested\"}, inplace=True)\n",
        "  x_table.rename(columns={\"Unit_load (lbs/sq in)\": \"SpecimenMeasuredStrength\"}, inplace=True)\n",
        "\n",
        "  return x_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuLG1JqoNTX9"
      },
      "source": [
        "**Importing bulk data into the function:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_23kdo1NS36"
      },
      "outputs": [],
      "source": [
        "first_table = []\n",
        "log_table = []\n",
        "second_table = []\n",
        "time_format = ''\n",
        "df_log = []\n",
        "log_file = []\n",
        "\n",
        "\n",
        "# Importing the file folder and reading the files.\n",
        "folder_path = '/content/drive/MyDrive/Colab_Notebooks_Main/Capstone/Files_no_issue'\n",
        "\n",
        "\n",
        "# Creating dataframes to store lod files:\n",
        "df_log = pd.DataFrame(columns=['File Name', 'Comment'])\n",
        "log_file = pd.DataFrame(columns=['File Name', 'Comment'])                 \n",
        "\n",
        "\n",
        "\n",
        "for file in os.listdir(folder_path):\n",
        "  if file.endswith(\".pdf\"):\n",
        "    print(file)\n",
        "\n",
        "\n",
        "    # Skipping the files with age days other than 7 or 28 days,  and maintaining them in Log table\n",
        "    res = re.findall(r'\\(.*?\\)', file)\n",
        "    if not '7' in str(res):\n",
        "      if not '28' in str(res):\n",
        "\n",
        "        df_log = df_log.append({'File Name' : file, 'Comment' : 'Invalid file with Age days other than 7 or 28'}, ignore_index = True)\n",
        "\n",
        "        continue\n",
        "\n",
        "    path = folder_path + '/' + file\n",
        "\n",
        "    pdf = pdfplumber.open(path)\n",
        "    page = pdf.pages[0]\n",
        "    text = page.extract_text()\n",
        "\n",
        "\n",
        "\n",
        "    # Cheking the number of times the string 'Field Set' is repeated, to count the number of 'Concrete Field Inspection and Test Results' tables:\n",
        "    repeated = text.count(\"Field Set\")\n",
        "\n",
        "\n",
        "    # Importing data to the functions\n",
        "\n",
        "    extracted_first = extract_first_table(path, text)\n",
        "\n",
        "    extracted_second = extract_second_table(path, text)\n",
        "\n",
        "\n",
        "\n",
        "    # Appending data into tables:\n",
        "\n",
        "    first_table = extracted_first.append(first_table, ignore_index=True)\n",
        "\n",
        "    second_table = extracted_second.append(second_table, ignore_index=True)\n",
        "\n",
        "    log_file = pd.concat([log_file, df_log], axis=0, ignore_index=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Delteing duplicate entries from first_table (SpecimenFieldReport) with same Date_Molded and Lab_no\n",
        "    new_first_table = first_table.drop_duplicates(subset = ['Date_Molded', 'Lab_no', 'Time_Molded']).reset_index(drop = True)  \n",
        "\n",
        "    # Delteing duplicate entries from second_table (SpecimenCompressionTestResult):\n",
        "    new_second_table = second_table.drop_duplicates()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oPBD2IPrqTe"
      },
      "outputs": [],
      "source": [
        "# Calculating the time taken for data scrapping\n",
        "\n",
        "end = time.time()\n",
        "time_taken = end - start\n",
        "print('Code took ' ,end - start, ' seconds.')\n",
        "print('Code took ' ,time_taken/60, ' minutes.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLs28D9ZJ4zS"
      },
      "source": [
        "# ***EXPORTING DATA TO EXCEL:***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTafIgXM9wxn"
      },
      "outputs": [],
      "source": [
        "first_table.to_excel(r'C:\\Users\\ramgo\\Desktop\\Extracted data\\first_table.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63sYHKi2jiNP"
      },
      "outputs": [],
      "source": [
        "second_table.to_excel(r'C:\\Users\\ramgo\\Desktop\\Extracted data\\second_table.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE8IYEE-EGhu"
      },
      "outputs": [],
      "source": [
        "log_file.to_excel(r'C:\\Users\\ramgo\\Desktop\\Extracted data\\log_table.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_first_table.to_excel(r'C:\\Users\\ramgo\\Desktop\\Extracted data\\new_first_table.xlsx', index=False)"
      ],
      "metadata": {
        "id": "YlHxydyndeWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_second_table.to_excel(r'C:\\Users\\ramgo\\Desktop\\Extracted data\\new_second_table.xlsx', index=False)"
      ],
      "metadata": {
        "id": "14h_zPfJdeSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}